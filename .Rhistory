ml_model[["results"]] <- ml_model[["results"]] %>%
left_join(auc_values, by = "cp")
#Tuning parameter (select combination with top AUC)
tune = which.max(ml_model$results$AUC)
ml_model$bestTune = ml_model$bestTune %>%
mutate(cp = ml_model$results$cp[tune])
#Configure resamples to have the AUCs only using tuned parameter
ml_model$resample = ml_model$resample[order(ml_model$resample$Resample),] #Order resamples (just in case) to match with correct AUCs from prediction object
auc = c()
for (i in 1:nrow(ml_model$resample)){
auc_val = ml_model$pred %>%
filter(cp == as.numeric(ml_model$bestTune$cp),
Resample == ml_model$resample$Resample[i]) %>%
pull(AUC) %>% #AUC per resample is the same
unique()
auc = c(auc, auc_val)
}
ml_model$resample = ml_model$resample %>%
mutate(AUC = auc)
predictions = predict(ml_model, newdata = training_df, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(CART = yes)  #Predictions of model (already ordered)
}else if(names(models)[i]=="LASSO"){ ################## Regularized Lasso
## Integrate AUCs into prediction matrix
ml_model$pred = ml_model$pred %>% #Calculate resamples AUC scores
group_by(Resample, alpha, lambda) %>%
mutate(AUC = calculate_auc_resample(obs, yes)) %>%
ungroup()
## Integrate AUCs into results per parameter
auc_values = ml_model$pred %>%
group_by(alpha, lambda) %>%
summarise(AUC = mean(AUC, na.rm = TRUE), .groups = 'drop')
ml_model[["results"]] <- ml_model[["results"]] %>%
left_join(auc_values, by = c("alpha", "lambda"))
#Tuning parameter (select combination with top AUC)
tune = which.max(ml_model$results$AUC)
ml_model$bestTune = ml_model$bestTune %>%
mutate(alpha = ml_model$results$alpha[tune],
lambda = ml_model$results$lambda[tune])
#Configure resamples to have the AUCs only using tuned parameter
ml_model$resample = ml_model$resample[order(ml_model$resample$Resample),] #Order resamples (just in case) to match with correct AUCs from prediction object
auc = c()
for (i in 1:nrow(ml_model$resample)){
auc_val = ml_model$pred %>%
filter(alpha == as.numeric(ml_model$bestTune$alpha),
lambda == as.numeric(ml_model$bestTune$lambda),
Resample == ml_model$resample$Resample[i]) %>%
pull(AUC) %>% #AUC per resample is the same
unique()
auc = c(auc, auc_val)
}
ml_model$resample = ml_model$resample %>%
mutate(AUC = auc)
predictions = predict(ml_model, newdata = training_df, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(LASSO = yes)  #Predictions of model (already ordered)
}else if(names(models)[i]=="RIDGE"){   ################## Ridge regression
## Integrate AUCs into prediction matrix
ml_model$pred = ml_model$pred %>% #Calculate resamples AUC scores
group_by(Resample, alpha, lambda) %>%
mutate(AUC = calculate_auc_resample(obs, yes)) %>%
ungroup()
## Integrate AUCs into results per parameter
auc_values = ml_model$pred %>%
group_by(alpha, lambda) %>%
summarise(AUC = mean(AUC, na.rm = TRUE), .groups = 'drop')
ml_model[["results"]] <- ml_model[["results"]] %>%
left_join(auc_values, by = c("alpha", "lambda"))
#Tuning parameter (select combination with top AUC)
tune = which.max(ml_model$results$AUC)
ml_model$bestTune = ml_model$bestTune %>%
mutate(alpha = ml_model$results$alpha[tune],
lambda = ml_model$results$lambda[tune])
#Configure resamples to have the AUCs only using tuned parameter
ml_model$resample = ml_model$resample[order(ml_model$resample$Resample),] #Order resamples (just in case) to match with correct AUCs from prediction object
auc = c()
for (i in 1:nrow(ml_model$resample)){
auc_val = ml_model$pred %>%
filter(alpha == as.numeric(ml_model$bestTune$alpha),
lambda == as.numeric(ml_model$bestTune$lambda),
Resample == ml_model$resample$Resample[i]) %>%
pull(AUC) %>% #AUC per resample is the same
unique()
auc = c(auc, auc_val)
}
ml_model$resample = ml_model$resample %>%
mutate(AUC = auc)
predictions = predict(ml_model, newdata = training_df, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(RIDGE = yes)  #Predictions of model (already ordered)
}else if(names(models)[i] == "SVM_radial"){ ################## Support Vector Machine with Radial Kernel
## Integrate AUCs into prediction matrix
ml_model$pred = ml_model$pred %>% #Calculate resamples AUC scores
group_by(Resample, sigma, C) %>%
mutate(AUC = calculate_auc_resample(obs, yes)) %>%
ungroup()
## Integrate AUCs into results per parameter
auc_values = ml_model$pred %>%
group_by(sigma, C) %>%
summarise(AUC = mean(AUC, na.rm = TRUE), .groups = 'drop')
ml_model[["results"]] <- ml_model[["results"]] %>%
left_join(auc_values, by = c("sigma", "C"))
#Tuning parameter (select combination with top AUC)
tune = which.max(ml_model$results$AUC)
ml_model$bestTune = ml_model$bestTune %>%
mutate(sigma = ml_model$results$sigma[tune],
C = ml_model$results$C[tune])
#Configure resamples to have the AUCs only using tuned parameter
ml_model$resample = ml_model$resample[order(ml_model$resample$Resample),] #Order resamples (just in case) to match with correct AUCs from prediction object
auc = c()
for (i in 1:nrow(ml_model$resample)){
auc_val = ml_model$pred %>%
filter(sigma == as.numeric(ml_model$bestTune$sigma),
C == as.numeric(ml_model$bestTune$C),
Resample == ml_model$resample$Resample[i]) %>%
pull(AUC) %>% #AUC per resample is the same
unique()
auc = c(auc, auc_val)
}
ml_model$resample = ml_model$resample %>%
mutate(AUC = auc)
predictions = predict(ml_model, newdata = training_df, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(SVM_radial = yes)  #Predictions of model (already ordered)
}else if(names(models)[i] == "SVM_linear"){
## Integrate AUCs into prediction matrix
ml_model$pred = ml_model$pred %>% #Calculate resamples AUC scores
group_by(Resample, C) %>%
mutate(AUC = calculate_auc_resample(obs, yes)) %>%
ungroup()
## Integrate AUCs into results per parameter
auc_values = ml_model$pred %>%
group_by(C) %>%
summarise(AUC = mean(AUC, na.rm = TRUE), .groups = 'drop')
ml_model[["results"]] <- ml_model[["results"]] %>%
left_join(auc_values, by = "C")
#Tuning parameter (select combination with top AUC)
tune = which.max(ml_model$results$AUC)
ml_model$bestTune = ml_model$bestTune %>%
mutate(C = ml_model$results$C[tune])
#Configure resamples to have the AUCs only using tuned parameter
ml_model$resample = ml_model$resample[order(ml_model$resample$Resample),] #Order resamples (just in case) to match with correct AUCs from prediction object
auc = c()
for (i in 1:nrow(ml_model$resample)){
auc_val = ml_model$pred %>%
filter(C == as.numeric(ml_model$bestTune$C),
Resample == ml_model$resample$Resample[i]) %>%
pull(AUC) %>% #AUC per resample is the same
unique()
auc = c(auc, auc_val)
}
ml_model$resample = ml_model$resample %>%
mutate(AUC = auc)
################## Support Vector Machine with Linear Kernel
predictions = predict(ml_model, newdata = training_df, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(SVM_linear = yes)  #Predictions of model (already ordered)
}
models[[i]] = ml_model
ml.predictions[[i]] = predictions
}
return(list(Models = models, ML_predictions = ml.predictions))
}
compute.k_fold_CV = function(model, k_folds, n_rep, stacking = F, boruta, boruta_iterations = NULL, fix_boruta = NULL, tentative = F, boruta_threshold = NULL, file_name = NULL, cv_metric, return){
######### Feature selection across folds
if(boruta == T){
cat("Feature selection using Boruta...............................................................\n\n")
# Feature selection using Boruta
res_boruta = feature.selection.boruta(model, iterations = boruta_iterations, fix = fix_boruta, file_name = file_name, doParallel = F, workers=NULL, threshold = boruta_threshold, return = return)
if(tentative == F){
if(length(res_boruta$Confirmed) <= 1){ #No enough features selected for training model
message("No enough features selected for training a model")
results = list()
return(results)
}else{
cat("\nKeeping only features confirmed in more than 80% of the times for training...............................................................\n\n")
cat("If you want to consider also tentative features, please specify tentative = T in the parameters.\n\n")
train_data = model[,colnames(model)%in%res_boruta$Confirmed, drop = F] %>%
mutate(target = model$target)
}
}else{
sum_features = length(res_boruta$Confirmed) + length(res_boruta$Tentative)
if(sum_features <= 1){
message("No enough features selected for training a model")
results = list()
return(results)
}else{
cat("Keeping features confirmed and tentative in more than 80% of the times for training...............................................................\n\n")
train_data = model[,colnames(model)%in%c(res_boruta$Confirmed, res_boruta$Tentative), drop = F] %>%
mutate(target = model$target)
}
}
rm(res_boruta) #Clean memory
gc()
}else{
train_data = model
}
rm(model) #Clean memory
gc()
cat("Training machine learning model...............................................................\n\n")
######### Machine Learning models
metric <- "Accuracy" #metric to use for selecting best methods (default: Accuracy -- for AUC see below and parameter must be equal to cv_metric = "AUC")
######### Stratify K fold cross-validation
#folds <- createFolds(train_data[,'target'], k = k_folds, returnTrain = T, list = T) #this for single folds
multifolds <- createMultiFolds(train_data[,'target'], k = k_folds, times = n_rep) #repeated folds
trainControl <- trainControl(index = multifolds, method="repeatedcv", number=k_folds, repeats=n_rep, verboseIter = F, allowParallel = F, classProbs = TRUE, savePredictions=T)
##################################################### ML models
#To do: Re-calculate accuracy values based on tuning parameters optimized by the cv AUC - now the values are based on accuracy! be careful
################## Bagged CART
fit.treebag <- train(target~., data = train_data, method = "treebag", metric = metric,trControl = trainControl)
predictions.bag <- data.frame(predict(fit.treebag$finalModel, newdata = train_data, type = "prob")) %>% #Predictions using tuned model
select(yes) %>%
rename(BAG = yes)
################## RF
fit.rf <- train(target~., data = train_data, method = "rf", metric = metric,trControl = trainControl)
predictions.rf = data.frame(predict(fit.rf$finalModel, newdata = train_data, type = "prob"))[,"yes", drop=F]  %>%
select(yes) %>%
rename(RF = yes) #Predictions of model (already ordered)
################## C5.0
fit.c50 <- train(target~., data = train_data, method = "C5.0", metric = metric,trControl = trainControl)
predictions.c50 = data.frame(predict(fit.c50$finalModel, newdata = train_data, type = "prob"))[,"yes", drop=F]  %>%
select(yes) %>%
rename(C50 = yes)  #Predictions of model (already ordered)
################## LG - Logistic Regression
fit.glm <- train(target~., data = train_data, method="glm", metric=metric,trControl=trainControl)
predictions.glm = predict(fit.glm, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(GLM = yes)  #Predictions of model (already ordered)
################## LDA - Linear Discriminate Analysis
fit.lda <- train(target~., data = train_data, method="lda", metric=metric,trControl=trainControl)
predictions.lda = predict(fit.lda, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(LDA = yes)  #Predictions of model (already ordered)
################## GLMNET - Regularized Logistic Regression (Elastic net)
fit.glmnet <- train(target~., data = train_data, method="glmnet", metric=metric,trControl=trainControl)
predictions.glmnet = predict(fit.glmnet, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(GLMNET = yes)  #Predictions of model (already ordered)
################## KNN - k-Nearest Neighbors
fit.knn <- train(target~., data = train_data, method="knn", metric=metric,trControl=trainControl)
predictions.knn = predict(fit.knn, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(KNN = yes) #Predictions of model (already ordered)
################## CART - Classification and Regression Trees (CART),
fit.cart <- train(target~., data = train_data, method="rpart", metric=metric,trControl=trainControl)
predictions.cart = predict(fit.cart, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(CART = yes)  #Predictions of model (already ordered)
################## Regularized Lasso
fit.lasso <- train(target~., data = train_data, method="glmnet", metric=metric,trControl=trainControl, tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 20)))
predictions.lasso = predict(fit.lasso, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(LASSO = yes)  #Predictions of model (already ordered)
################## Ridge regression
fit.ridge <- train(target~., data = train_data, method="glmnet", metric=metric,trControl=trainControl, tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 20)))
predictions.ridge = predict(fit.ridge, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(RIDGE = yes)  #Predictions of model (already ordered)
################## Support Vector Machine with Radial Kernel
fit.svm_radial <- train(target ~ ., data = train_data, method = "svmRadial", metric = metric, trControl = trainControl)
predictions.svm_radial = predict(fit.svm_radial, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(SVM_radial = yes)  #Predictions of model (already ordered)
################## Support Vector Machine with Linear Kernel
fit.svm_linear <- train(target ~ ., data = train_data, method = "svmLinear", metric = metric, trControl = trainControl)
predictions.svm_linear = predict(fit.svm_linear, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(SVM_linear = yes)  #Predictions of model (already ordered)
############################################################## Save models
ensembleResults <- list(BAG = fit.treebag,
RF = fit.rf,
C50 = fit.c50,
GLM = fit.glm,
LDA = fit.lda,
KNN = fit.knn,
CART = fit.cart,
GLMNET = fit.glmnet,
LASSO = fit.lasso,
RIDGE = fit.ridge,
SVM_radial = fit.svm_radial,
SVM_linear = fit.svm_linear)
model_predictions = list(BAG = predictions.bag,
RF = predictions.rf,
C50 = predictions.c50,
GLM = predictions.glm,
LDA = predictions.lda,
KNN = predictions.knn,
CART = predictions.cart,
GLMNET = predictions.glmnet,
LASSO = predictions.lasso,
RIDGE = predictions.ridge,
SVM_radial = predictions.svm_radial,
SVM_linear = predictions.svm_linear)
##Tune parameters based on AUC
if(cv_metric == "AUC"){
tuning_results = compute_AUC_hyperparameters_tuning(ensembleResults, model_predictions, train_data)
ensembleResults = tuning_results[[1]]
model_predictions = tuning_results[[2]]
}
#Remove models with same predictions across samples (not able to make distinction)
model_predictions <- lapply(model_predictions, function(df) {
df = df %>%
select(where(~ n_distinct(.) > 1))
if(ncol(df) == 0){
df = NULL
}
return(df)
})
model_predictions = Filter(Negate(is.null), model_predictions) #Discard not useful predictions
ensembleResults = ensembleResults[names(model_predictions)] #Discard not useful models based on predictions
model_predictions = do.call(cbind, model_predictions) #Join as data frame
rm(fit.treebag, fit.rf, fit.c50, fit.glm, fit.lda, fit.knn, fit.cart, fit.glmnet, fit.lasso, fit.ridge, fit.svm_radial, fit.svm_linear, multifolds)
gc()
if(stacking){
features = colnames(train_data)[colnames(train_data) != "target"]
#Base models using ML models with best accuracy from each family
if(cv_metric == "AUC"){
base_models = compute_cv_AUC(ensembleResults, base_models = T)
}else{
base_models = compute_cv_accuracy(ensembleResults, base_models = T)
}
features_predictions = model_predictions %>%
t() %>%
data.frame() %>%
rownames_to_column("Models") %>%
filter(grepl(paste0("\\b(", paste(base_models$Base_models, collapse = "|"), ")\\b"), Models)) %>%
column_to_rownames("Models") %>%
t() %>%
data.frame()
meta_features = cbind(features_predictions, "true_label" = train_data$target)
meta_learner <- train(true_label ~ ., data = meta_features, method = "glmnet", trControl = trainControl) #Staking based on simple logistic regression
#Base models using ALL ML models
meta_features_all = cbind(model_predictions, "true_label" = train_data$target)
meta_learner_all <- train(true_label ~ ., data = meta_features_all, method = "glmnet", trControl = trainControl) #Staking based on simple logistic regression
cat("Meta-learners ML model based on GLM\n")
output = list("Features" = features, "Meta_learners" = list("simple" = meta_learner, "all" = meta_learner_all), "Base_models" = base_models$Base_models, "ML_models" = ensembleResults)
}else{
features = colnames(train_data)[colnames(train_data) != "target"] #Extract features used for model training
#metrics = compute_cv_accuracy(ensembleResults, file_name, return = F)
metrics = compute_cv_AUC(ensembleResults, file_name, return = F)
top_model = metrics[["Top_model"]]
model = ensembleResults[[top_model]]
cat("Best ML model found: ", top_model, "\n")
cat("Returning model trained\n")
output = list("Features" = features, "Model" = model, "ML_Models" = ensembleResults)
}
return(output)
}
compute.RMSE_k_fold_CV = function(train_data, k_folds, n_rep, stacking = F, file_name = NULL, return){
cat("Training machine learning model...............................................................\n\n")
######### Machine Learning models
metric <- "RMSE" #metric to use for selecting best methods (default: Accuracy -- for AUC see below and parameter must be equal to cv_metric = "AUC")
######### Stratify K fold cross-validation
#folds <- createFolds(train_data[,'target'], k = k_folds, returnTrain = T, list = T) #this for single folds
multifolds <- createMultiFolds(train_data[,'target'], k = k_folds, times = n_rep) #repeated folds
trainControl <- trainControl(index = multifolds, method="repeatedcv", number=k_folds, repeats=n_rep, verboseIter = F, allowParallel = F, classProbs = TRUE, savePredictions=T)
##################################################### ML models
#To do: Re-calculate accuracy values based on tuning parameters optimized by the cv AUC - now the values are based on accuracy! be careful
################## Bagged CART
fit.treebag <- train(target~., data = train_data, method = "treebag", metric = metric,trControl = trainControl)
predictions.bag <- data.frame(predict(fit.treebag$finalModel, newdata = train_data, type = "prob")) %>% #Predictions using tuned model
select(yes) %>%
rename(BAG = yes)
################## RF
fit.rf <- train(target~., data = train_data, method = "rf", metric = metric,trControl = trainControl)
predictions.rf = data.frame(predict(fit.rf$finalModel, newdata = train_data, type = "prob"))[,"yes", drop=F]  %>%
select(yes) %>%
rename(RF = yes) #Predictions of model (already ordered)
################## C5.0
fit.c50 <- train(target~., data = train_data, method = "C5.0", metric = metric,trControl = trainControl)
predictions.c50 = data.frame(predict(fit.c50$finalModel, newdata = train_data, type = "prob"))[,"yes", drop=F]  %>%
select(yes) %>%
rename(C50 = yes)  #Predictions of model (already ordered)
################## LG - Logistic Regression
fit.glm <- train(target~., data = train_data, method="glm", metric=metric,trControl=trainControl)
predictions.glm = predict(fit.glm, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(GLM = yes)  #Predictions of model (already ordered)
################## LDA - Linear Discriminate Analysis
fit.lda <- train(target~., data = train_data, method="lda", metric=metric,trControl=trainControl)
predictions.lda = predict(fit.lda, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(LDA = yes)  #Predictions of model (already ordered)
################## GLMNET - Regularized Logistic Regression (Elastic net)
fit.glmnet <- train(target~., data = train_data, method="glmnet", metric=metric,trControl=trainControl)
predictions.glmnet = predict(fit.glmnet, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(GLMNET = yes)  #Predictions of model (already ordered)
################## KNN - k-Nearest Neighbors
fit.knn <- train(target~., data = train_data, method="knn", metric=metric,trControl=trainControl)
predictions.knn = predict(fit.knn, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(KNN = yes) #Predictions of model (already ordered)
################## CART - Classification and Regression Trees (CART),
fit.cart <- train(target~., data = train_data, method="rpart", metric=metric,trControl=trainControl)
predictions.cart = predict(fit.cart, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(CART = yes)  #Predictions of model (already ordered)
################## Regularized Lasso
fit.lasso <- train(target~., data = train_data, method="glmnet", metric=metric,trControl=trainControl, tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, length = 20)))
predictions.lasso = predict(fit.lasso, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(LASSO = yes)  #Predictions of model (already ordered)
################## Ridge regression
fit.ridge <- train(target~., data = train_data, method="glmnet", metric=metric,trControl=trainControl, tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 20)))
predictions.ridge = predict(fit.ridge, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(RIDGE = yes)  #Predictions of model (already ordered)
################## Support Vector Machine with Radial Kernel
fit.svm_radial <- train(target ~ ., data = train_data, method = "svmRadial", metric = metric, trControl = trainControl)
predictions.svm_radial = predict(fit.svm_radial, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(SVM_radial = yes)  #Predictions of model (already ordered)
################## Support Vector Machine with Linear Kernel
fit.svm_linear <- train(target ~ ., data = train_data, method = "svmLinear", metric = metric, trControl = trainControl)
predictions.svm_linear = predict(fit.svm_linear, newdata = train_data, type = "prob")[,"yes", drop=F]  %>%
select(yes) %>%
rename(SVM_linear = yes)  #Predictions of model (already ordered)
############################################################## Save models
ensembleResults <- list(BAG = fit.treebag,
RF = fit.rf,
C50 = fit.c50,
GLM = fit.glm,
LDA = fit.lda,
KNN = fit.knn,
CART = fit.cart,
GLMNET = fit.glmnet,
LASSO = fit.lasso,
RIDGE = fit.ridge,
SVM_radial = fit.svm_radial,
SVM_linear = fit.svm_linear)
model_predictions = list(BAG = predictions.bag,
RF = predictions.rf,
C50 = predictions.c50,
GLM = predictions.glm,
LDA = predictions.lda,
KNN = predictions.knn,
CART = predictions.cart,
GLMNET = predictions.glmnet,
LASSO = predictions.lasso,
RIDGE = predictions.ridge,
SVM_radial = predictions.svm_radial,
SVM_linear = predictions.svm_linear)
#Remove models with same predictions across samples (not able to make distinction)
model_predictions <- lapply(model_predictions, function(df) {
df = df %>%
select(where(~ n_distinct(.) > 1))
if(ncol(df) == 0){
df = NULL
}
return(df)
})
model_predictions = Filter(Negate(is.null), model_predictions) #Discard not useful predictions
ensembleResults = ensembleResults[names(model_predictions)] #Discard not useful models based on predictions
model_predictions = do.call(cbind, model_predictions) #Join as data frame
rm(fit.treebag, fit.rf, fit.c50, fit.glm, fit.lda, fit.knn, fit.cart, fit.glmnet, fit.lasso, fit.ridge, fit.svm_radial, fit.svm_linear, multifolds)
gc()
if(stacking){
features = colnames(train_data)[colnames(train_data) != "target"]
#Base models using ML models with best accuracy from each family
if(cv_metric == "AUC"){
base_models = compute_cv_AUC(ensembleResults, base_models = T)
}else{
base_models = compute_cv_accuracy(ensembleResults, base_models = T)
}
features_predictions = model_predictions %>%
t() %>%
data.frame() %>%
rownames_to_column("Models") %>%
filter(grepl(paste0("\\b(", paste(base_models$Base_models, collapse = "|"), ")\\b"), Models)) %>%
column_to_rownames("Models") %>%
t() %>%
data.frame()
meta_features = cbind(features_predictions, "true_label" = train_data$target)
meta_learner <- train(true_label ~ ., data = meta_features, method = "glmnet", trControl = trainControl) #Staking based on simple logistic regression
#Base models using ALL ML models
meta_features_all = cbind(model_predictions, "true_label" = train_data$target)
meta_learner_all <- train(true_label ~ ., data = meta_features_all, method = "glmnet", trControl = trainControl) #Staking based on simple logistic regression
cat("Meta-learners ML model based on GLM\n")
output = list("Features" = features, "Meta_learners" = list("simple" = meta_learner, "all" = meta_learner_all), "Base_models" = base_models$Base_models, "ML_models" = ensembleResults)
}else{
features = colnames(train_data)[colnames(train_data) != "target"] #Extract features used for model training
#metrics = compute_cv_accuracy(ensembleResults, file_name, return = F)
metrics = compute_cv_AUC(ensembleResults, file_name, return = F)
top_model = metrics[["Top_model"]]
model = ensembleResults[[top_model]]
cat("Best ML model found: ", top_model, "\n")
cat("Returning model trained\n")
output = list("Features" = features, "Model" = model, "ML_Models" = ensembleResults)
}
return(output)
}
source("ML_functions.R")
renv::init()
renv::activate()
renv::snapshot()
renv::status()
renv::snapshot()
install.packages('Matrix')
install.packages("Matrix")
install.packages(Matrix)
library(Matrix)
renv::snapshot()
renv::snapshot()
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages('subselect')
install.packages("caret"
install.packages("caret")
install.packages("ggstatsplot")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix", repos = "http://R-Forge.R-project.org")
