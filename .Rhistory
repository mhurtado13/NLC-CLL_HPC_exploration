labs(title = 'Residuals Plot', x = 'Predicted Values', y = 'Residuals') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
scale_y_continuous(limits = c(min(data$residuals) - 5, max(data$residuals) + 5)) +  # Adjust limits
geom_smooth(method = 'loess', color = 'blue', se = FALSE, linetype = "solid") +  # Add LOESS curve
annotate("text", x = min(data$Predicted) * 0.3, y = max(data$residuals)*0.2,
label = paste("Mean Residuals:", round(mean(data$residuals), 2)),
color = "black", size = 4.5, hjust = 0)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
evaluate_prediction = function(predicted, observed){
######Metrics
# Calculate RMSE
rmse <- sqrt(mean((observed - predicted)^2))
# Calculate MAE
mae <- mean(abs(observed - predicted))
# Calculate R-squared
ss_res <- sum((observed - predicted) ^ 2)
ss_tot <- sum((observed - mean(observed)) ^ 2)
r_squared <- 1 - (ss_res / ss_tot)
metrics = data.frame(RMSE = rmse,
MAE = mae,
R_square = r_squared)
data <- data.frame(
Actual = observed,
Predicted = predicted
)
# Fit a linear regression model
model <- lm(Actual ~ Predicted, data = data)
# Regression plot
p1 <- ggplot(data, aes(x = Predicted, y = Actual)) +
geom_point(color = 'blue', size = 2, alpha = 0.6) +  # Scatter plot
geom_smooth(method = 'lm', color = 'red', se = FALSE, linetype = "dashed") +  # Regression line
labs(title = 'Regression Plot', x = 'Predicted Values', y = 'Actual Values') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.8,
label = paste("RMSE:", round(rmse, 2)), color = "black", size = 4, hjust = 0) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.7,
label = paste("R-squared:", round(r_squared, 3)), color = "black", size = 4, hjust = 0)
print(p1)
# Residuals analysis
data$residuals <- residuals(model)
p2 <- ggplot(data, aes(x = Predicted, y = residuals)) +
geom_point(color = 'green', alpha = 0.6, size = 2) +  # Residuals scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Horizontal line at y=0
labs(title = 'Residuals Plot', x = 'Predicted Values', y = 'Residuals') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
scale_y_continuous(limits = c(min(data$residuals) - 5, max(data$residuals) + 5)) +  # Adjust limits
geom_smooth(method = 'loess', color = 'blue', se = FALSE, linetype = "solid") +  # Add LOESS curve
annotate("text", x = min(data$Predicted) * 0.3, y = max(data$residuals)*0.8,
label = paste("Mean Residuals:", round(mean(data$residuals), 2)),
color = "black", size = 4.5, hjust = 0)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
evaluate_prediction = function(predicted, observed){
######Metrics
# Calculate RMSE
rmse <- sqrt(mean((observed - predicted)^2))
# Calculate MAE
mae <- mean(abs(observed - predicted))
# Calculate R-squared
ss_res <- sum((observed - predicted) ^ 2)
ss_tot <- sum((observed - mean(observed)) ^ 2)
r_squared <- 1 - (ss_res / ss_tot)
metrics = data.frame(RMSE = rmse,
MAE = mae,
R_square = r_squared)
data <- data.frame(
Actual = observed,
Predicted = predicted
)
# Fit a linear regression model
model <- lm(Actual ~ Predicted, data = data)
# Regression plot
p1 <- ggplot(data, aes(x = Predicted, y = Actual)) +
geom_point(color = 'blue', size = 2, alpha = 0.6) +  # Scatter plot
geom_smooth(method = 'lm', color = 'red', se = FALSE, linetype = "dashed") +  # Regression line
labs(title = 'Regression Plot', x = 'Predicted Values', y = 'Actual Values') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.8,
label = paste("RMSE:", round(rmse, 2)), color = "black", size = 4, hjust = 0) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.7,
label = paste("R-squared:", round(r_squared, 3)), color = "black", size = 4, hjust = 0)
print(p1)
# Residuals analysis
data$residuals <- residuals(model)
p2 <- ggplot(data, aes(x = Predicted, y = residuals)) +
geom_point(color = 'green', alpha = 0.6, size = 2) +  # Residuals scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Horizontal line at y=0
labs(title = 'Residuals Plot', x = 'Predicted Values', y = 'Residuals') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
scale_y_continuous(limits = c(min(data$residuals) - 5, max(data$residuals) + 5)) +  # Adjust limits
geom_smooth(method = 'loess', color = 'blue', se = FALSE, linetype = "solid") +  # Add LOESS curve
annotate("text", x = min(data$Predicted) * 0.3, y = min(data$residuals)*0.8,
label = paste("Mean Residuals:", round(mean(data$residuals), 2)),
color = "black", size = 4.5, hjust = 0)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
evaluate_prediction = function(predicted, observed){
######Metrics
# Calculate RMSE
rmse <- sqrt(mean((observed - predicted)^2))
# Calculate MAE
mae <- mean(abs(observed - predicted))
# Calculate R-squared
ss_res <- sum((observed - predicted) ^ 2)
ss_tot <- sum((observed - mean(observed)) ^ 2)
r_squared <- 1 - (ss_res / ss_tot)
metrics = data.frame(RMSE = rmse,
MAE = mae,
R_square = r_squared)
data <- data.frame(
Actual = observed,
Predicted = predicted
)
# Fit a linear regression model
model <- lm(Actual ~ Predicted, data = data)
# Regression plot
p1 <- ggplot(data, aes(x = Predicted, y = Actual)) +
geom_point(color = 'blue', size = 2, alpha = 0.6) +  # Scatter plot
geom_smooth(method = 'lm', color = 'red', se = FALSE, linetype = "dashed") +  # Regression line
labs(title = 'Regression Plot', x = 'Predicted Values', y = 'Actual Values') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.8,
label = paste("RMSE:", round(rmse, 2)), color = "black", size = 4, hjust = 0) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.7,
label = paste("R-squared:", round(r_squared, 3)), color = "black", size = 4, hjust = 0)
print(p1)
# Residuals analysis
data$residuals <- residuals(model)
p2 <- ggplot(data, aes(x = Predicted, y = residuals)) +
geom_point(color = 'green', alpha = 0.6, size = 2) +  # Residuals scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Horizontal line at y=0
labs(title = 'Residuals Plot', x = 'Predicted Values', y = 'Residuals') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
scale_y_continuous(limits = c(min(data$residuals) - 5, max(data$residuals) + 5)) +  # Adjust limits
geom_smooth(method = 'loess', color = 'blue', se = FALSE, linetype = "solid") +  # Add LOESS curve
annotate("text", x = min(data$Predicted) * 0.3, y = min(data$residuals)*0.2,
label = paste("Mean Residuals:", round(mean(data$residuals), 2)),
color = "black", size = 4.5, hjust = 0)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
evaluate_prediction = function(predicted, observed){
######Metrics
# Calculate RMSE
rmse <- sqrt(mean((observed - predicted)^2))
# Calculate MAE
mae <- mean(abs(observed - predicted))
# Calculate R-squared
ss_res <- sum((observed - predicted) ^ 2)
ss_tot <- sum((observed - mean(observed)) ^ 2)
r_squared <- 1 - (ss_res / ss_tot)
metrics = data.frame(RMSE = rmse,
MAE = mae,
R_square = r_squared)
data <- data.frame(
Actual = observed,
Predicted = predicted
)
# Fit a linear regression model
model <- lm(Actual ~ Predicted, data = data)
# Regression plot
p1 <- ggplot(data, aes(x = Predicted, y = Actual)) +
geom_point(color = 'blue', size = 2, alpha = 0.6) +  # Scatter plot
geom_smooth(method = 'lm', color = 'red', se = FALSE, linetype = "dashed") +  # Regression line
labs(title = 'Regression Plot', x = 'Predicted Values', y = 'Actual Values') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.8,
label = paste("RMSE:", round(rmse, 2)), color = "black", size = 4, hjust = 0) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.7,
label = paste("R-squared:", round(r_squared, 3)), color = "black", size = 4, hjust = 0)
print(p1)
# Residuals analysis
data$residuals <- residuals(model)
p2 <- ggplot(data, aes(x = Predicted, y = residuals)) +
geom_point(color = 'green', alpha = 0.6, size = 2) +  # Residuals scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Horizontal line at y=0
labs(title = 'Residuals Plot', x = 'Predicted Values', y = 'Residuals') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
scale_y_continuous(limits = c(min(data$residuals) - 5, max(data$residuals) + 5)) +  # Adjust limits
geom_smooth(method = 'loess', color = 'blue', se = FALSE, linetype = "solid") +  # Add LOESS curve
annotate("text", x = min(data$Predicted) * 0.3, y = max(data$residuals)*0.95,
label = paste("Mean Residuals:", round(mean(data$residuals), 2)),
color = "black", size = 4.5, hjust = 0)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
evaluate_prediction = function(predicted, observed){
######Metrics
# Calculate RMSE
rmse <- sqrt(mean((observed - predicted)^2))
# Calculate MAE
mae <- mean(abs(observed - predicted))
# Calculate R-squared
ss_res <- sum((observed - predicted) ^ 2)
ss_tot <- sum((observed - mean(observed)) ^ 2)
r_squared <- 1 - (ss_res / ss_tot)
metrics = data.frame(RMSE = rmse,
MAE = mae,
R_square = r_squared)
data <- data.frame(
Actual = observed,
Predicted = predicted
)
# Fit a linear regression model
model <- lm(Actual ~ Predicted, data = data)
# Regression plot
p1 <- ggplot(data, aes(x = Predicted, y = Actual)) +
geom_point(color = 'blue', size = 2, alpha = 0.6) +  # Scatter plot
geom_smooth(method = 'lm', color = 'red', se = FALSE, linetype = "dashed") +  # Regression line
labs(title = 'Regression Plot', x = 'Predicted Values', y = 'Actual Values') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.8,
label = paste("RMSE:", round(rmse, 2)), color = "black", size = 4, hjust = 0) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.7,
label = paste("R-squared:", round(r_squared, 3)), color = "black", size = 4, hjust = 0)
print(p1)
# Residuals analysis
data$residuals <- residuals(model)
p2 <- ggplot(data, aes(x = Predicted, y = residuals)) +
geom_point(color = 'green', alpha = 0.6, size = 2) +  # Residuals scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Horizontal line at y=0
labs(title = 'Residuals Plot',
x = 'Predicted Values',
y = 'Residuals',
subtitle = paste("Mean of Residuals =", round(mean_residuals, 2))) +  # Mean as subtitle
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
plot.subtitle = element_text(hjust = 0.5, size = 12, face = "italic"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
evaluate_prediction = function(predicted, observed){
######Metrics
# Calculate RMSE
rmse <- sqrt(mean((observed - predicted)^2))
# Calculate MAE
mae <- mean(abs(observed - predicted))
# Calculate R-squared
ss_res <- sum((observed - predicted) ^ 2)
ss_tot <- sum((observed - mean(observed)) ^ 2)
r_squared <- 1 - (ss_res / ss_tot)
metrics = data.frame(RMSE = rmse,
MAE = mae,
R_square = r_squared)
data <- data.frame(
Actual = observed,
Predicted = predicted
)
# Fit a linear regression model
model <- lm(Actual ~ Predicted, data = data)
# Regression plot
p1 <- ggplot(data, aes(x = Predicted, y = Actual)) +
geom_point(color = 'blue', size = 2, alpha = 0.6) +  # Scatter plot
geom_smooth(method = 'lm', color = 'red', se = FALSE, linetype = "dashed") +  # Regression line
labs(title = 'Regression Plot', x = 'Predicted Values', y = 'Actual Values') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.8,
label = paste("RMSE:", round(rmse, 2)), color = "black", size = 4, hjust = 0) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.7,
label = paste("R-squared:", round(r_squared, 3)), color = "black", size = 4, hjust = 0)
print(p1)
# Residuals analysis
data$residuals <- residuals(model)
p2 <- ggplot(data, aes(x = Predicted, y = residuals)) +
geom_point(color = 'green', alpha = 0.6, size = 2) +  # Residuals scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Horizontal line at y=0
labs(title = 'Residuals Plot',
x = 'Predicted Values',
y = 'Residuals',
subtitle = paste("Mean of Residuals =", round(mean(data$residuals), 2))) +  # Mean as subtitle
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
plot.subtitle = element_text(hjust = 0.5, size = 12, face = "italic"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
evaluate_prediction = function(predicted, observed){
######Metrics
# Calculate RMSE
rmse <- sqrt(mean((observed - predicted)^2))
# Calculate MAE
mae <- mean(abs(observed - predicted))
# Calculate R-squared
ss_res <- sum((observed - predicted) ^ 2)
ss_tot <- sum((observed - mean(observed)) ^ 2)
r_squared <- 1 - (ss_res / ss_tot)
metrics = data.frame(RMSE = rmse,
MAE = mae,
R_square = r_squared)
data <- data.frame(
Actual = observed,
Predicted = predicted
)
# Fit a linear regression model
model <- lm(Actual ~ Predicted, data = data)
# Regression plot
p1 <- ggplot(data, aes(x = Predicted, y = Actual)) +
geom_point(color = 'blue', size = 2, alpha = 0.6) +  # Scatter plot
geom_smooth(method = 'lm', color = 'red', se = FALSE, linetype = "dashed") +  # Regression line
labs(title = 'Regression Plot', x = 'Predicted Values', y = 'Actual Values') +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.8,
label = paste("RMSE:", round(rmse, 2)), color = "black", size = 4, hjust = 0) +
annotate("text", x = max(data$Predicted) * 0.1, y = max(data$Actual) * 0.7,
label = paste("R-squared:", round(r_squared, 3)), color = "black", size = 4, hjust = 0)
print(p1)
# Residuals analysis
data$residuals <- residuals(model)
p2 <- ggplot(data, aes(x = Predicted, y = residuals)) +
geom_point(color = 'green', alpha = 0.6, size = 2) +  # Residuals scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Horizontal line at y=0
labs(title = 'Residuals Plot',
x = 'Predicted Values',
y = 'Residuals',
subtitle = paste("Mean of Residuals =", round(mean(data$residuals), 2))) +  # Mean as subtitle
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
plot.subtitle = element_text(hjust = 0.5, size = 12),
axis.title.x = element_text(size = 12),
axis.title.y = element_text(size = 12)
)
print(p2)
return(metrics)
}
prediction = compute.prediction.stacked(meta_learner, testing_set, target, training[["ML_models"]], training[["Base_models"]])
# Generate 60 samples with 10 features (random normal values)
features <- as.data.frame(matrix(rnorm(4000 * 10), nrow = 4000, ncol = 10))
# Name the feature columns
colnames(features) <- paste0("feature_", 1:10)
# Generate the target column (random normal values)
features$target <- rnorm(4000)
View(features)
x = compute.ML(features, "target", 0.8, stack = T, seed = 123, file_name = "Test")
install.packages("kernlab")
install.packages("kernlab")
install.packages("kernlab")
install.packages(kernlab)
install.packages("kernlab")
install.packages("~/Downloads/kernlab_0.9-33.tar.gz", repos = NULL, type = "source")
install.packages("mgcv")
install.packages("~/Downloads/mgcv_1.9-1.tar.gz", repos = NULL, type = "source")
install.packages("MASS")
install.packages("~/Downloads/MASS_7.3-61.tar.gz", repos = NULL, type = "source")
install.packages("~/Downloads/MASS_7.3-53.tar.gz", repos = NULL, type = "source")
source("ML_functions.R")
# Generate 60 samples with 10 features (random normal values)
features <- as.data.frame(matrix(rnorm(100 * 10), nrow = 100, ncol = 10))
# Name the feature columns
colnames(features) <- paste0("feature_", 1:10)
# Generate the target column (random normal values)
features$target <- rnorm(100)
x = compute.ML(features, "target", 0.8, stack = T, seed = 123, file_name = "Test"))
samples = features
target_variable = "target"
partition = 0.8
stack = T
seed = 123
file_name = "Test"
samples = samples %>%
rename_with(~"target", all_of(target_variable)) #In case target variable is with a different name
set.seed(seed)
# Do partition
index = createDataPartition(samples[,"target"], times = 1, p = partition, list = FALSE)
# Train cohort
training_set = samples[index,]
# Test cohort
testing_set = samples[-index,]
training = compute_surrogate_models(training_set, k_folds = 5, n_rep = 100, file_name = file_name))
train_data = training_set
k_folds = 5
n_rep = 100
######### Machine Learning models
metric <- "RMSE" #metric to use for selecting best methods (default: Accuracy -- for AUC see below and parameter must be equal to cv_metric = "AUC")
### Stratify K fold cross-validation
multifolds <- createMultiFolds(train_data[,'target'], k = k_folds, times = n_rep) #repeated folds
trainControl <- trainControl(index = multifolds, method="repeatedcv", number=k_folds, repeats=n_rep, verboseIter = F, allowParallel = F, savePredictions=T)
polynomial_model <- list(
type = "Regression",
library = NULL,  # No need to load any external libraries
loop = NULL,
# Define the hyperparameter grid (polynomial degree)
parameters = data.frame(
parameter = 'degree',
class = 'numeric',
label = 'Polynomial Degree'
),
# Grid function for tuning the polynomial degree
grid = function(x, y, len = NULL, search = "grid") {
data.frame(degree = seq(1, 5, by = 1))  # Tune degrees from 1 to 5
},
# Fitting function: applies polynomial regression
fit = function(x, y, wts, param, lev, last, classProbs, ...) {
lm(y ~ poly(as.matrix(x), degree = param$degree))
},
# Predict function: predicts based on the model
predict = function(modelFit, newdata, submodels = NULL) {
predict(modelFit, as.matrix(newdata))
},
# We don't need probability predictions for regression
prob = NULL
)
View(train_data)
polynomial_fit <- train(target ~ ., data = train_data, method = polynomial_model, metric = metric, trControl = trainControl, tuneLength = 3)
View(polynomial_model)
predictions.polynomial <- data.frame(Polynomial = predict(polynomial_fit, newdata = train_data))
### Stratify K fold cross-validation
multifolds <- createMultiFolds(train_data[,'target'], k = k_folds, times = 1) #repeated folds
trainControl <- trainControl(index = multifolds, method="repeatedcv", number=k_folds, repeats=1, verboseIter = F, allowParallel = F, savePredictions=T)
# Generate 60 samples with 10 features (random normal values)
features <- as.data.frame(matrix(rnorm(100 * 3), nrow = 100, ncol = 3))
# Name the feature columns
colnames(features) <- paste0("feature_", 1:3)
# Generate the target column (random normal values)
features$target <- rnorm(100)
train_data = features
######### Machine Learning models
metric <- "RMSE" #metric to use for selecting best methods (default: Accuracy -- for AUC see below and parameter must be equal to cv_metric = "AUC")
### Stratify K fold cross-validation
multifolds <- createMultiFolds(train_data[,'target'], k = k_folds, times = 1) #repeated folds
trainControl <- trainControl(index = multifolds, method="repeatedcv", number=k_folds, repeats=1, verboseIter = F, allowParallel = F, savePredictions=T)
polynomial_fit <- train(target ~ ., data = train_data, method = polynomial_model, metric = metric, trControl = trainControl, tuneLength = 3)
train_data[,'target']
View(train_data)
### Stratify K fold cross-validation
multifolds <- createMultiFolds(train_data[,'target'], k = k_folds, times = 1) #repeated folds
trainControl <- trainControl(index = multifolds, method="repeatedcv", number=k_folds, repeats=1, verboseIter = F, allowParallel = F, savePredictions=T)
polynomial_fit <- train(target ~ ., data = train_data, method = polynomial_model, metric = metric, trControl = trainControl, tuneLength = 3)
polynomial_fit <- train(target ~ ., data = train_data, method = polynomial_model, metric = metric, trControl = trainControl, tuneLength = 2)
##################################################### ML models
################## Bagged CART
fit.treebag <- train(target~., data = train_data, method = "treebag", metric = metric,trControl = trainControl)
polynomial_fit <- train(target ~ ., data = train_data, method = polynomial_model, metric = metric, trControl = trainControl, tuneLength = 2)
